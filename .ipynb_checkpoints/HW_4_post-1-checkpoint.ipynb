{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "In this assignment, we'll use what we learned in preprocessing module to compare ChatGPT-generated text with human-generated answers. A dataset with 200 questions and answers has been provided for you to use. The dataset can be found at https://huggingface.co/datasets/Hello-SimpleAI/HC3.\n",
    "\n",
    "\n",
    "Please follow the instruction below to do the assessment step by step and answer all analysis questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>chatgpt_answer</th>\n",
       "      <th>human_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happens if a parking ticket is lost / des...</td>\n",
       "      <td>If a parking ticket is lost or destroyed befor...</td>\n",
       "      <td>In my city you also get something by mail to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why the waves do n't interfere ? first , I 'm ...</td>\n",
       "      <td>Interference is the phenomenon that occurs whe...</td>\n",
       "      <td>They do actually . That 's why a microwave ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it possible to influence a company's action...</td>\n",
       "      <td>Yes, it is possible to influence a company's a...</td>\n",
       "      <td>Yes and no. This really should be taught at ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why do taxpayers front the bill for sports sta...</td>\n",
       "      <td>Sports stadiums are usually built with public ...</td>\n",
       "      <td>That 's the bargaining chip that team owners u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why do clothing stores generally have a ton of...</td>\n",
       "      <td>There are a few reasons why clothing stores ma...</td>\n",
       "      <td>Your observation is almost certainly a matter ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What happens if a parking ticket is lost / des...   \n",
       "1  why the waves do n't interfere ? first , I 'm ...   \n",
       "2  Is it possible to influence a company's action...   \n",
       "3  Why do taxpayers front the bill for sports sta...   \n",
       "4  Why do clothing stores generally have a ton of...   \n",
       "\n",
       "                                      chatgpt_answer  \\\n",
       "0  If a parking ticket is lost or destroyed befor...   \n",
       "1  Interference is the phenomenon that occurs whe...   \n",
       "2  Yes, it is possible to influence a company's a...   \n",
       "3  Sports stadiums are usually built with public ...   \n",
       "4  There are a few reasons why clothing stores ma...   \n",
       "\n",
       "                                        human_answer  \n",
       "0  In my city you also get something by mail to t...  \n",
       "1  They do actually . That 's why a microwave ove...  \n",
       "2  Yes and no. This really should be taught at ju...  \n",
       "3  That 's the bargaining chip that team owners u...  \n",
       "4  Your observation is almost certainly a matter ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C://Users//Rahul//Documents//Rahul Rajpurohit Assignments//BIA 660//HW_4_Rahul//qa.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Tokenize function\n",
    "\n",
    "Define a function `tokenize(docs, lemmatized = True, remove_stopword = True, remove_punct = True)`  as follows:\n",
    "   - Take three parameters: \n",
    "       - `docs`: a list of documents (e.g. questions)\n",
    "       - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
    "       - `remove_stopword`: an optional bookean parameter to remove stop words. The default value is True (i.e. remove stop words). \n",
    "   - Split each input document into unigrams and also clean up tokens as follows:\n",
    "       - if `lemmatized` is turned on, lemmatize all unigrams.\n",
    "       - if `remove_stopword` is set to True, remove all stop words.\n",
    "       - if `remove_punct` is set to True, remove all punctuation tokens.\n",
    "       - remove all empty tokens and lowercase all the tokens.\n",
    "   - Return the list of tokens obtained for each document after all the processing. \n",
    "   \n",
    "(Hint: you can use spacy package for this task. For reference, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# load the English language model in spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_a_doc(doc, nlp, lemmatized=True, remove_stopword=True, remove_punct=True):\n",
    "    # initialize an empty list to store the tokens\n",
    "    tokens = []\n",
    "    # create a spacy document from the input text\n",
    "    doc = nlp(doc)\n",
    "    for token in doc:\n",
    "        # if remove_punct is True, exclude punctuation tokens\n",
    "        if remove_punct and token.is_punct:\n",
    "            continue\n",
    "        # if remove_stopword is True, exclude stop words\n",
    "        if remove_stopword and token.is_stop:\n",
    "            continue\n",
    "        # if lemmatized is True, lemmatize the token\n",
    "        if lemmatized:\n",
    "            lemma = token.lemma_.lower().strip()  # lemmatize and convert to lowercase\n",
    "            tokens.append(lemma)\n",
    "        else:\n",
    "            tokens.append(token.text.lower().strip())  # convert to lowercase and append to the list\n",
    "    # exclude empty tokens\n",
    "    tokens = [t for t in tokens if t]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(docs, lemmatized=True, remove_stopword=True, remove_punct=True):\n",
    "    # initialize an empty list to store the tokens for each document\n",
    "    tokenized_docs = []\n",
    "    for doc in docs:\n",
    "        # tokenize each document and append to the list\n",
    "        tokenized_doc = tokenize_a_doc(doc, nlp, lemmatized=lemmatized, remove_stopword=remove_stopword, remove_punct=remove_punct)\n",
    "        tokenized_docs.append(tokenized_doc)\n",
    "    return tokenized_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function with different parameter configuration and observe the differences in the resulting tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happens if a parking ticket is lost / destroyed before the owner is aware of the ticket , and it goes unpaid ? I 've always been curious . Please explain like I'm five.\n",
      "\n",
      "1.lemmatized=True, remove_stopword=False, remove_punct = True:\n",
      " [['what', 'happen', 'if', 'a', 'parking', 'ticket', 'be', 'lose', 'destroy', 'before', 'the', 'owner', 'be', 'aware', 'of', 'the', 'ticket', 'and', 'it', 'go', 'unpaid', 'i', 've', 'always', 'be', 'curious', 'please', 'explain', 'like', 'i', 'be', 'five']]\n",
      "\n",
      "2.lemmatized=True, remove_stopword=True, remove_punct = True:\n",
      " [['happen', 'parking', 'ticket', 'lose', 'destroy', 'owner', 'aware', 'ticket', 'go', 'unpaid', 've', 'curious', 'explain', 'like']]\n",
      "\n",
      "3.lemmatized=False, remove_stopword=False, remove_punct = True:\n",
      " [['what', 'happens', 'if', 'a', 'parking', 'ticket', 'is', 'lost', 'destroyed', 'before', 'the', 'owner', 'is', 'aware', 'of', 'the', 'ticket', 'and', 'it', 'goes', 'unpaid', 'i', 've', 'always', 'been', 'curious', 'please', 'explain', 'like', 'i', \"'m\", 'five']]\n",
      "\n",
      "4.lemmatized=False, remove_stopword=False, remove_punct = False:\n",
      " [['what', 'happens', 'if', 'a', 'parking', 'ticket', 'is', 'lost', '/', 'destroyed', 'before', 'the', 'owner', 'is', 'aware', 'of', 'the', 'ticket', ',', 'and', 'it', 'goes', 'unpaid', '?', 'i', \"'\", 've', 'always', 'been', 'curious', '.', 'please', 'explain', 'like', 'i', \"'m\", 'five', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For simplicity, We will test on document\n",
    "\n",
    "print(data[\"question\"].iloc[0] + \"\\n\")\n",
    "\n",
    "print(f\"1.lemmatized=True, remove_stopword=False, remove_punct = True:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=True, remove_stopword=False, remove_punct = True)}\\n\")\n",
    "\n",
    "print(f\"2.lemmatized=True, remove_stopword=True, remove_punct = True:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=True, remove_stopword=True, remove_punct = True)}\\n\")\n",
    "\n",
    "print(f\"3.lemmatized=False, remove_stopword=False, remove_punct = True:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=False, remove_stopword=False, remove_punct = True)}\\n\")\n",
    "\n",
    "print(f\"4.lemmatized=False, remove_stopword=False, remove_punct = False:\\n \\\n",
    "{tokenize(data['question'].iloc[0:1], lemmatized=False, remove_stopword=False, remove_punct = False)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Sentiment Analysis\n",
    "\n",
    "\n",
    "Let's check if there is any difference in sentiment between ChatGPT-generated and human-generated answers.\n",
    "\n",
    "\n",
    "Define a function `compute_sentiment(generated, reference, pos, neg )` as follows:\n",
    "- take four parameters:\n",
    "    - `gen_tokens` is the tokenized ChatGPT-generated answers by the `tokenize` function in Q1.\n",
    "    - `ref_tokens` is the tokenized human-generated answers by the `tokenize` function in Q1.\n",
    "    - `pos` (`neg`) is the lists of positive (negative) words, which can be find in Canvas preprocessing module.\n",
    "- for each ChatGPT-generated or human-generated answer, compute the sentiment as `(#pos - #neg )/(#pos + #neg)`, where `#pos`(`#neg`) is the number of positive (negative) words found in each answer. If an answer contains none of the positive or negative words, set the sentiment to 0.\n",
    "- return the sentiment of ChatGPT-generated and human-generated answers as two columns of DataFrame.\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how sentiment results change.\n",
    "- Do you think, in general, which tokenization configuration should be used? Why does this combination make the most senese?\n",
    "- Do you think, overall, ChatGPT-generated answers are more posive or negative than human-generated ones? Use data to support your conclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def compute_sentiment(gen_tokens, ref_tokens, pos, neg ):\n",
    "    \n",
    "    # Define function to compute sentiment for a single answer\n",
    "    def compute_single_sentiment(tokens):\n",
    "        pos_count = Counter([token for token in tokens if token in pos])\n",
    "        neg_count = Counter([token for token in tokens if token in neg])\n",
    "        if not pos_count and not neg_count:\n",
    "            return 0\n",
    "        return (sum(pos_count.values()) - sum(neg_count.values())) / (sum(pos_count.values()) + sum(neg_count.values()))\n",
    "\n",
    "    # Compute sentiment for generated and reference tokens\n",
    "    gen_sentiments = [compute_single_sentiment(tokens) for tokens in gen_tokens]\n",
    "    ref_sentiments = [compute_single_sentiment(tokens) for tokens in ref_tokens]\n",
    "\n",
    "    # Create DataFrame with results\n",
    "    result = pd.DataFrame({'Generated Sentiment': gen_sentiments, 'Reference Sentiment': ref_sentiments})\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=False, remove_punct = False)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=False, remove_punct = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abundance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abundant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0         a+\n",
       "1     abound\n",
       "2    abounds\n",
       "3  abundance\n",
       "4   abundant"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-faced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2-faces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abominable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0     2-faced\n",
       "1     2-faces\n",
       "2    abnormal\n",
       "3     abolish\n",
       "4  abominable"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = pd.read_csv(\"positive-words.txt\", header = None)\n",
    "pos.head()\n",
    "\n",
    "neg = pd.read_csv(\"negative-words.txt\", header = None)\n",
    "neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated Sentiment</th>\n",
       "      <th>Reference Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.777778</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Generated Sentiment  Reference Sentiment\n",
       "0             0.000000            -0.500000\n",
       "1            -0.777778             0.076923\n",
       "2             0.666667             0.200000\n",
       "3             1.000000             0.200000\n",
       "4             0.600000            -0.333333"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = compute_sentiment(gen_tokens, \n",
    "                           ref_tokens, \n",
    "                           pos[0].values,\n",
    "                           neg[0].values)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14665715815970656"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10403.0, 0.0010660004805700114)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "(result['Generated Sentiment'] - result['Reference Sentiment']).mean()\n",
    "\n",
    "res = wilcoxon(result['Generated Sentiment'] - result['Reference Sentiment'], alternative='greater')\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=True, remove_punct = False)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=True, remove_punct = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated Sentiment</th>\n",
       "      <th>Reference Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Generated Sentiment  Reference Sentiment\n",
       "0             0.000000            -0.500000\n",
       "1            -0.777778             0.000000\n",
       "2             0.666667             0.111111\n",
       "3             1.000000             0.200000\n",
       "4             0.600000            -0.333333"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = compute_sentiment(gen_tokens, \n",
    "                           ref_tokens, \n",
    "                           pos[0].values,\n",
    "                           neg[0].values)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1587041451334569"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10451.0, 0.0004926382674291639)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "(result['Generated Sentiment'] - result['Reference Sentiment']).mean()\n",
    "\n",
    "res = wilcoxon(result['Generated Sentiment'] - result['Reference Sentiment'], alternative='greater')\n",
    "res.statistic, res.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obversed that the each pos and neg file contains the root form of words. So, lemmatization is not necessary here. We choose to set remove_stopwords as true because, in a tokenized form the stopwords have no significance. remove_punct is set false, because few words might have \"-\" in it. Also, after setting, lemmatization and remove_punct as 'False' and remove_stopwords as 'True', we get pvalue which is less than the significant value, so here in our case, we can reject the null hypothesis and we can conclude that there is statistically significant difference between the two groups and the ChatGPT generated answers are positive than the human generated ones as it's mean is greater than that of human generated ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Performance Evaluation\n",
    "\n",
    "\n",
    "Next, we evaluate how accurate the ChatGPT-generated answers are, compared to the human-generated answers. One widely used method is to calculate the `precision` and `recall` of n-grams. For simplicity, we only calculate bigrams here. You can try unigram, trigram, or n-grams in the same way.\n",
    "\n",
    "\n",
    "Define a funtion `bigram_precision_recall(gen_tokens, ref_tokens)` as follows:\n",
    "- take two parameters:\n",
    "    - `gen_tokens` is the tokenized ChatGPT-generated answers by the `tokenize` function in Q1.\n",
    "    - `ref_tokens` is the tokenized human answers by the `tokenize` function in Q1.\n",
    "- generate bigrams from each tokenized document in `gen_tokens` and `ref_tokens`\n",
    "- for each pair of ChatGPT-generated and human answers, find the overlapping bigrams between them\n",
    "- compute `precision` as the number of overlapping bigrams divided by the total number of bigrams from the ChatGPT-generated answer. In other words, the bigram is considered as a predicted value. The `precision` measures the percentage of correctly generated bigrams out of all generated bigrams.\n",
    "- compute `recall` as the number of overlapping bigrams divided by the total number of bigrams from the human answer. In other words, the `recall` measures the percentage of bigrams from the human answer can be successfully retrieved.\n",
    "- return the precision and recall for each pair of answers.\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how precison and recall change.\n",
    "- Do you think, in general, which tokenization configuration should be used? Why does this combination make the most senese?\n",
    "- Do you think, overall, ChatGPT is able to mimic human in answering these questions?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def bigram_precision_recall(gen_tokens, ref_tokens):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for gen_doc, ref_doc in zip(gen_tokens, ref_tokens):\n",
    "        gen_bigrams = set(ngrams(gen_doc, 2))\n",
    "        ref_bigrams = set(ngrams(ref_doc, 2))\n",
    "        overlap = gen_bigrams.intersection(ref_bigrams)\n",
    "        precision.append(len(overlap) / len(gen_bigrams) if len(gen_bigrams) > 0 else 0)\n",
    "        recall.append(len(overlap) / len(ref_bigrams) if len(ref_bigrams) > 0 else 0)\n",
    "    result = pd.DataFrame({'precision': precision, 'recall': recall})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=False, remove_stopword=True, remove_punct = False)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=False, remove_stopword=True, remove_punct = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.012048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall\n",
       "0   0.000000  0.000000\n",
       "1   0.035294  0.012048\n",
       "2   0.015625  0.007143\n",
       "3   0.000000  0.000000\n",
       "4   0.018519  0.055556"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bigram_precision_recall(gen_tokens, \n",
    "                                 ref_tokens)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.035239\n",
       "recall       0.056134\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[[\"precision\", \"recall\"]].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=True, remove_punct = False)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=True, remove_punct = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.016064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision    recall\n",
       "0   0.000000  0.000000\n",
       "1   0.047619  0.016064\n",
       "2   0.046875  0.021429\n",
       "3   0.000000  0.000000\n",
       "4   0.038835  0.111111"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = bigram_precision_recall(gen_tokens, \n",
    "                                 ref_tokens)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "precision    0.040448\n",
       "recall       0.062346\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[[\"precision\", \"recall\"]].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the lemmatization to true to get better precision and recall value. But in our case, both precision and recall is relatively low, so we conclude that ChatGPT is not able to mimic the human generated answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Compute TF-IDF\n",
    "\n",
    "Define a function `compute_tf_idf(tokenized_docs)` as follows: \n",
    "- Take paramter `tokenized_docs`, i.e., a list of tokenized documents by `tokenize` function in Q1\n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: feel free to reuse the code segment in Lecture Notes (II))\n",
    "- Return the smoothed normalized `tf_idf` array, where each row stands for a document and each column denotes a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_tfidf(tokenized_docs):\n",
    "    # Step 1: Create the word set\n",
    "    word_set = set()\n",
    "    for doc in tokenized_docs:\n",
    "        for word in doc:\n",
    "            word_set.add(word)\n",
    "    word_list = list(word_set)\n",
    "    num_words = len(word_list)\n",
    "    num_docs = len(tokenized_docs)\n",
    "\n",
    "    # Step 2: Create the document-term matrix\n",
    "    doc_term_mat = np.zeros((num_docs, num_words))\n",
    "    for i, doc in enumerate(tokenized_docs):\n",
    "        for word in doc:\n",
    "            j = word_list.index(word)\n",
    "            doc_term_mat[i, j] += 1\n",
    "\n",
    "    # Step 3: Compute the term frequency and inverse document frequency\n",
    "    tf_mat = np.zeros((num_docs, num_words))\n",
    "    idf_vec = np.zeros(num_words)\n",
    "    for j in range(num_words):\n",
    "        idf_vec[j] = np.log(num_docs / np.count_nonzero(doc_term_mat[:, j]))\n",
    "        for i in range(num_docs):\n",
    "            tf_mat[i, j] = doc_term_mat[i, j] / np.sum(doc_term_mat[i, :])\n",
    "\n",
    "    # Step 4: Compute the tf-idf matrix\n",
    "    tfidf_mat = np.multiply(tf_mat, idf_vec)\n",
    "\n",
    "    # Step 5: Smooth and normalize the tf-idf matrix\n",
    "    smoothed_tf_idf = np.zeros((num_docs, num_words))\n",
    "    for i in range(num_docs):\n",
    "        tfidf_max = np.max(tfidf_mat[i, :])\n",
    "        smoothed_tf_idf[i, :] = (0.5 + 0.5*tfidf_mat[i, :]/tfidf_max) / np.sqrt(np.sum(np.square(0.5 + 0.5*tfidf_mat[i, :]/tfidf_max)))\n",
    "\n",
    "    return smoothed_tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different tokenization options to see how these options affect TFIDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.lemmatized=True, remove_stopword=False, remove_punct = True\n",
      " Shape: (200, 1439)\n",
      "\n",
      "2.lemmatized=True, remove_stopword=True, remove_punct = True:\n",
      " Shape: (200, 1272)\n",
      "\n",
      "3.lemmatized=False, remove_stopword=False, remove_punct = True:\n",
      " Shape: (200, 1643)\n",
      "\n",
      "4.lemmatized=False, remove_stopword=False, remove_punct = False:\n",
      " Shape: (200, 1665)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test tfidf generation using questions\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=True, remove_stopword=False, remove_punct = True)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"1.lemmatized=True, remove_stopword=False, remove_punct = True\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"2.lemmatized=True, remove_stopword=True, remove_punct = True:\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=False, remove_stopword=False, remove_punct = True)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"3.lemmatized=False, remove_stopword=False, remove_punct = True:\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n",
    "\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=False, remove_stopword=False, remove_punct = False)\n",
    "dtm = compute_tfidf(question_tokens)\n",
    "print(f\"4.lemmatized=False, remove_stopword=False, remove_punct = False:\\n \\\n",
    "Shape: {dtm.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Assess similarity. \n",
    "\n",
    "\n",
    "Define a function `assess_similarity(question_tokens, gen_tokens, ref_tokens)`  as follows: \n",
    "- Take three inputs:\n",
    "   - `question_tokens`: tokenized questions by `tokenize` function in Q1\n",
    "   - `gen_tokens`: tokenized ChatGPT-generated answers by `tokenize` function in Q1\n",
    "   - `ref_tokens`: tokenized human answers by `tokenize` function in Q1\n",
    "- Concatenate these three token lists into a single list to form a corpus\n",
    "- Calculate the smoothed normalized tf_idf matrix for the concatenated list using the `compute_tfidf` function defined in Q3.\n",
    "- Split the tf_idf matrix into sub-matrices corresponding to `question_tokens`, `gen_tokens`, and `ref_tokens` respectively\n",
    "- For each question, find its similarities to the paired ChatGPT-generated answer and human answer.\n",
    "- For each pair of ChatGPT-generated answer and human answer, find their similarity\n",
    "- Print out the following:\n",
    "    - the question which has the largest similarity to the ChatGPT-generated answer.\n",
    "    - the question which has the largest similarity to the human answer.\n",
    "    - the pair of ChatGPT-generated and human answers which have the largest similarity.\n",
    "- Return a DataFrame with the three columns for the similarities among questions and answers.\n",
    "\n",
    "\n",
    "\n",
    "Analysis: \n",
    "- Try different tokenization parameter configurations (lemmatized, remove_stopword, remove_punct), and observe how similarities change.\n",
    "- Based on similarity, do you think ChatGPT-generate answers are more relevant to questions than human answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def assess_similarity(question_tokens, gen_tokens, ref_tokens):\n",
    "    # Concatenate the three token lists into a single corpus\n",
    "    corpus = question_tokens + gen_tokens + ref_tokens\n",
    "    \n",
    "    # Calculate the smoothed normalized tf-idf matrix for the corpus\n",
    "    tfidf_mat = compute_tfidf(corpus)\n",
    "    \n",
    "    # Split the tf-idf matrix into sub-matrices for questions, generated answers, and reference answers\n",
    "    q_tfidf_mat = tfidf_mat[:len(question_tokens)]\n",
    "    gen_tfidf_mat = tfidf_mat[len(question_tokens):len(question_tokens)+len(gen_tokens)]\n",
    "    ref_tfidf_mat = tfidf_mat[len(question_tokens)+len(gen_tokens):]\n",
    "    \n",
    "    # Find the similarity between each question and its paired generated and reference answers\n",
    "    q_gen_similarities = cosine_similarity(q_tfidf_mat, gen_tfidf_mat)\n",
    "    q_ref_similarities = cosine_similarity(q_tfidf_mat, ref_tfidf_mat)\n",
    "    \n",
    "    # Find the similarity between each pair of generated and reference answers\n",
    "    gen_ref_similarities = cosine_similarity(gen_tfidf_mat, ref_tfidf_mat)\n",
    "    \n",
    "    # Find the indices of the questions with highest similarity to generated and reference answers\n",
    "    max_q_gen_pair = [question_tokens[i] for i in q_gen_similarities.argmax(axis=0)]\n",
    "    max_q_ref_pair = [question_tokens[i] for i in q_ref_similarities.argmax(axis=0)]\n",
    "    \n",
    "    # Find the indices of the pair of generated and reference answers with highest similarity\n",
    "    max_gen_ref_pair = gen_tokens[gen_ref_similarities.argmax(axis=None)//len(ref_tokens)], ref_tokens[gen_ref_similarities.argmax(axis=None)%len(ref_tokens)]\n",
    "    \n",
    "    # Create a DataFrame with the three columns for the similarities among each questions and answers\n",
    "    similarities_df = pd.DataFrame({\n",
    "        'question_to_generated_answer': q_gen_similarities.max(axis=1),\n",
    "        'question_to_reference_answer': q_ref_similarities.max(axis=1),\n",
    "        'generated_to_reference_answer': gen_ref_similarities.max(axis=1)\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # Print out the results\n",
    "    new_data = pd.concat([data, similarities_df], axis=1)\n",
    "    print(\"Question with highest similarity to generated answer:\") \n",
    "    print(new_data.iloc[new_data[\"question_to_generated_answer\"].values.argmax(), 1])\n",
    "    print(\"Question with the largest similarity to the human answer.:\")\n",
    "    print(new_data.iloc[new_data[\"question_to_reference_answer\"].values.argmax(), 1])\n",
    "    print(\"The pair of ChatGPT-generated and human answers which have the largest similarity.\")\n",
    "    print(new_data.iloc[new_data[\"generated_to_reference_answer\"].values.argmax(), 1:3])\n",
    "\n",
    "    \n",
    "    return new_data, similarities_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = tokenize(data[\"chatgpt_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "ref_tokens = tokenize(data[\"human_answer\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n",
    "question_tokens = tokenize(data[\"question\"], lemmatized=True, remove_stopword=True, remove_punct = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question with highest similarity to generated answer:\n",
      "Sure! People's taste buds are all different, which means that some people might like different foods than other people. This is because everyone's taste buds process flavors differently. Some people might think a certain food tastes really good, while others might think it tastes bad. It's all just a matter of personal preference. So if you don't like a certain food, it doesn't mean there's something wrong with you. It just means that your taste buds don't enjoy that particular flavor as much as someone else's might.\n",
      "Question with the largest similarity to the human answer.:\n",
      "Customer lifetime value (CLV) is a measure of the total value that a customer will generate for a business over the course of their relationship with the company. It is an important concept in marketing and customer relationship management, as it helps businesses to understand the long-term value of their customers and to allocate resources accordingly.\n",
      "\n",
      "\n",
      "\n",
      "To calculate CLV, a business will typically consider factors such as the amount of money that a customer spends over time, the length of time they remain a customer, and the profitability of the products or services they purchase. The CLV of a customer can be used to help a business make decisions about how to allocate marketing resources, how to price products and services, and how to retain and improve relationships with valuable customers.\n",
      "\n",
      "\n",
      "\n",
      "Some businesses may also consider other factors when calculating CLV, such as the potential for a customer to refer other customers to the business, or the potential for a customer to engage with the business in non-monetary ways (e.g. through social media or other forms of word-of-mouth marketing).\n",
      "The pair of ChatGPT-generated and human answers which have the largest similarity.\n",
      "chatgpt_answer    The Maya civilization, which flourished in Mes...\n",
      "human_answer      Maya numerals are a vigesimal ( base - twenty ...\n",
      "Name: 37, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_to_generated_answer</th>\n",
       "      <th>question_to_reference_answer</th>\n",
       "      <th>generated_to_reference_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.999612</td>\n",
       "      <td>0.999577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999864</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.999695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999557</td>\n",
       "      <td>0.999711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999710</td>\n",
       "      <td>0.999545</td>\n",
       "      <td>0.999696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999232</td>\n",
       "      <td>0.999156</td>\n",
       "      <td>0.999721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_to_generated_answer  question_to_reference_answer  \\\n",
       "0                      0.999797                      0.999612   \n",
       "1                      0.999864                      0.999758   \n",
       "2                      0.999657                      0.999557   \n",
       "3                      0.999710                      0.999545   \n",
       "4                      0.999232                      0.999156   \n",
       "\n",
       "   generated_to_reference_answer  \n",
       "0                       0.999577  \n",
       "1                       0.999695  \n",
       "2                       0.999711  \n",
       "3                       0.999696  \n",
       "4                       0.999721  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data, result = assess_similarity(question_tokens, gen_tokens, ref_tokens)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_to_generated_answer</th>\n",
       "      <th>question_to_reference_answer</th>\n",
       "      <th>generated_to_reference_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.999571</td>\n",
       "      <td>0.999559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.998977</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>0.998532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.999528</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.999488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.999653</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.999595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.999743</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.999691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_to_generated_answer  question_to_reference_answer  \\\n",
       "count                    200.000000                    200.000000   \n",
       "mean                       0.999625                      0.999571   \n",
       "std                        0.000171                      0.000170   \n",
       "min                        0.998977                      0.998980   \n",
       "25%                        0.999528                      0.999467   \n",
       "50%                        0.999653                      0.999607   \n",
       "75%                        0.999743                      0.999701   \n",
       "max                        0.999943                      0.999897   \n",
       "\n",
       "       generated_to_reference_answer  \n",
       "count                     200.000000  \n",
       "mean                        0.999559  \n",
       "std                         0.000193  \n",
       "min                         0.998532  \n",
       "25%                         0.999488  \n",
       "50%                         0.999595  \n",
       "75%                         0.999691  \n",
       "max                         0.999868  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the similarities we can say that ChatGPT answers are more relevant as mean of question_to_generated_answer is slighty greater than question_to_reference_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (Bonus): Further Analysis (Open question)\n",
    "\n",
    "\n",
    "- Can you find at least three significant differences between ChatGPT-generated and human answeres? Use data to support your answer.\n",
    "- Based on these differences, are you able to design a classifier to identify ChatGPT generated answers? Implement your ideas using traditional machine learning models, such as SVM, decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
